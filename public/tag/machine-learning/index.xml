<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Hasan Kurban, Ph.D.</title>
    <link>https://hasankurban.netlify.app/tag/machine-learning/</link>
      <atom:link href="https://hasankurban.netlify.app/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Code with  and  Blogdown
© Hasan Kurban, 2020</copyright><lastBuildDate>Mon, 11 Dec 2017 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://hasankurban.netlify.app/images/icon_hu20e5e5b4446f9fea2c18d55ef2678e39_24096_512x512_fill_lanczos_center_2.png</url>
      <title>Machine Learning</title>
      <link>https://hasankurban.netlify.app/tag/machine-learning/</link>
    </image>
    
    <item>
      <title>Iterative Machine Learning</title>
      <link>https://hasankurban.netlify.app/project/kmeans-project/</link>
      <pubDate>Mon, 11 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://hasankurban.netlify.app/project/kmeans-project/</guid>
      <description>&lt;p&gt;In this work, we have described an optimization approach that can be used over any iterative optimization
algorithms to improve their training run-time complexity. To the best of our knowledge, this is the first work that theoretically shows convergence of iterative algorithms over heap structure–instead of over a cost function. This approach is tested over k-means (KM) and expectation-maximization algorithm (EM-T). The experimental results show dramatic improvements over KM and EM-T training run-time through different kinds of testing: scale, dimension, and separability. Regarding cluster error, the traditional algorithms’ and our
extended algorithms’ performances are similar. For future work, clearly one obvious step is to add seeding to KM* drawing from both k-means++ and kd trees. Additionally,we are interested in the broader question of this approach to iterative converging algorithms. Further, are there better structures than heaps? Lastly, parallelization offers some new challenges, but also opportunities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Random Forest</title>
      <link>https://hasankurban.netlify.app/project/rf-project/</link>
      <pubDate>Sat, 27 Jun 2015 00:00:00 +0000</pubDate>
      <guid>https://hasankurban.netlify.app/project/rf-project/</guid>
      <description>&lt;p&gt;Random Forests have been used as effective ensemble models for classification. We present in this paper a new type of Random Forests (RFs) called Red(uced)-RF that adopts a new dynamic data reduction principle and a new voting mechanism called Priority Vote Weighting (PV) which improve accuracy, execution time and AUC values compared to Breiman’s RF. Red-RF also shows that the strength of a random forest can increase without noticeably increasing correlation between the trees. We then compare performance of Red-RF and Breiman’s RF in 8 experiments that involve classification problems with datasets of different sizes. Finally, we conduct two additional experiments that involve considerably larger datasets with one million points in each.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
